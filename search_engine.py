# search_engine.py

import os
import time
import openai
import faiss
import numpy as np
import pickle
from dotenv import load_dotenv
from tenacity import retry, stop_after_attempt, wait_exponential

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=OPENAI_API_KEY)

# FAISS ì¸ë±ìŠ¤ & ë©”íƒ€ë°ì´í„° ë¡œë“œ
print("ğŸ“š FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
index = faiss.read_index("notices.index")
with open("notices_metadata.pkl", "rb") as f:
    metadata = pickle.load(f)

# ì„ë² ë”© ìƒì„± í•¨ìˆ˜
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def get_embedding(text):
    response = client.embeddings.create(
        input=text,
        model="text-embedding-ada-002"
    )
    time.sleep(0.5)
    return response.data[0].embedding

# ê³µì§€ì‚¬í•­ ê²€ìƒ‰ í•¨ìˆ˜
def search_notices(query, top_k=3):
    query_embedding = get_embedding(query)
    query_vector = np.array([query_embedding]).astype('float32')
    distances, indices = index.search(query_vector, top_k)
    results = [metadata[idx] for idx in indices[0]]
    return distances, indices, results

# FastAPIìš© ë¬¸ë§¥ ì¶”ì¶œ í•¨ìˆ˜
def search_similar_documents(query, top_k=10):
    _, _, results = search_notices(query, top_k=top_k)
    context = ""
    field_names = set()
    for doc in results:
        context += "- ë¬¸ì„œ ì •ë³´:\n"
        for key, value in doc.items():
            if value:
                context += f"  {key}: {value}\n"
                field_names.add(key)
        context += "|\n"
    return context, field_names

# ìš”ì•½ í•¨ìˆ˜ (ì„ íƒì  ì‚¬ìš©)
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def get_llm_summary(query, results):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMì´ ë§¥ë½ì„ ê³ ë ¤í•´ ìš”ì•½í•˜ê³  ì¬ì •ë ¬í•©ë‹ˆë‹¤."""
    
    # ìµœëŒ€ ê¸¸ì´ ì œí•œ ì„¤ì • (characters ê¸°ì¤€, ì›í•˜ëŠ” ëŒ€ë¡œ ì¡°ì • ê°€ëŠ¥)
    MAX_CONTENT_LEN = 1000
    MAX_TABLE_LEN = 1500

    # results_text ìƒì„± (ë³¸ë¬¸ê³¼ í‘œ ë³„ë„ ê¸¸ì´ ì œí•œ ì ìš©)
    results_text = "\n\n".join([
        f"[{i+1}] ì œëª©: {r['title']}\n"
        f"ë‚ ì§œ: {r['date']}\n"
        f"ë‚´ìš©: "
        + (
            # ë³¸ë¬¸ contentì¸ ê²½ìš°
            (r.get('content', '')[:MAX_CONTENT_LEN] + ' ...(ìƒëµë¨)' if len(r.get('content', '')) > MAX_CONTENT_LEN else r.get('content', ''))
            if r['type'] == 'content'
            # í‘œ table_contentì¸ ê²½ìš°
            else (r.get('table_content', '')[:MAX_TABLE_LEN] + ' ...(ìƒëµë¨)' if len(r.get('table_content', '')) > MAX_TABLE_LEN else r.get('table_content', ''))
        )
        for i, r in enumerate(results)
    ])

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê³µì§€ì‚¬í•­ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ë¥¼ ì¬ì •ë ¬í•˜ê³ , ê° ê²°ê³¼ê°€ ì™œ ê´€ë ¨ìˆëŠ”ì§€ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.

        ì‚¬ìš©ì ì§ˆë¬¸: {query}

        ê²€ìƒ‰ ê²°ê³¼:
        {results_text}

        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        1. [ì¬ì •ë ¬ëœ ìˆœìœ„] ì œëª©: (ì œëª©)
        - ê´€ë ¨ì„± ì„¤ëª…: (ì´ ê²°ê³¼ê°€ ì™œ ê´€ë ¨ìˆëŠ”ì§€ ì„¤ëª…)
        2. [ì¬ì •ë ¬ëœ ìˆœìœ„] ì œëª©: (ì œëª©)
        - ê´€ë ¨ì„± ì„¤ëª…: (ì´ ê²°ê³¼ê°€ ì™œ ê´€ë ¨ìˆëŠ”ì§€ ì„¤ëª…)
        ...
        """

    # LLM í˜¸ì¶œ
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",  # í•„ìš”í•œ ê²½ìš° "gpt-4" ë¡œ ë³€ê²½ ê°€ëŠ¥
            messages=[{"role": "user", "content": prompt}]
        )
        time.sleep(1)  # Rate Limit ë°©ì§€
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ LLM ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
        raise