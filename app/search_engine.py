# app/search_engine.py

import os
from dotenv import load_dotenv

# LangChainì—ì„œ FAISSì™€ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document # LangChainì˜ Document ê°ì²´ íƒ€ì… íŒíŠ¸ìš©

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# --- 1. ê²½ë¡œ ì„¤ì • ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Vector DB ìƒì„± ì‹œ ì‚¬ìš©í–ˆë˜ ê²ƒê³¼ "ë™ì¼í•œ" ì„ë² ë”© ëª¨ë¸ì„ LangChain ë°©ì‹ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
# (ì´ ëª¨ë¸ì€ DBë¥¼ ë§Œë“¤ ë•Œ ì‚¬ìš©í•œ ëª¨ë¸ê³¼ ë°˜ë“œì‹œ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤!)
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

# --- 2. ê° Vector DBë¥¼ ë¡œë”©í•©ë‹ˆë‹¤ ---

# ê³µì§€ì‚¬í•­ DB ë¡œë”©
NOTICES_DB_DIR = os.path.join(BASE_DIR, '..', 'vector_store', 'notices')
notices_db = None # ì´ˆê¸°ê°’ ì„¤ì •
try:
    notices_db = FAISS.load_local(
        folder_path=NOTICES_DB_DIR,
        embeddings=embeddings,
        index_name="notices_title_index",  # ğŸš¨ 'notices_title_index.faiss' íŒŒì¼ ì´ë¦„ (í™•ì¥ì ì œì™¸)
        allow_dangerous_deserialization=True
    )
    print("âœ… Notices Vector DB ë¡œë”© ì„±ê³µ.")
except Exception as e:
    print(f"âŒ Notices Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")
    # vector_db = None  # ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ì œê±°

# ì·¨ì—… ì •ë³´ DB ë¡œë”©
JOBS_DB_DIR = os.path.join(BASE_DIR, '..', 'vector_store', 'jobs')
jobs_db = None # ì´ˆê¸°ê°’ ì„¤ì •
try:
    jobs_db = FAISS.load_local(
        folder_path=JOBS_DB_DIR,
        embeddings=embeddings,
        index_name="jobs_openai_index", # ğŸš¨ 'jobs_openai_index.faiss' íŒŒì¼ ì´ë¦„ (í™•ì¥ì ì œì™¸)
        allow_dangerous_deserialization=True
    )
    print("âœ… Jobs Vector DB ë¡œë”© ì„±ê³µ.")
except Exception as e:
    print(f"âŒ Jobs Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")
    # vector_db = None  # ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ì œê±°


# --- 3. ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ DBë¥¼ ì„ íƒí•˜ëŠ” ë¼ìš°í„° í•¨ìˆ˜ ---
def route_query_to_db(query: str) -> FAISS:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ Vector DBë¥¼ ì„ íƒí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    job_keywords = ["ì·¨ì—…", "ì¸í„´", "ì±„ìš©", "íšŒì‚¬", "ì§ë¬´", "ìì†Œì„œ", "ë©´ì ‘", "êµ¬ì¸", "ëª¨ì§‘", "ê³µê³ ", "ì¼ìë¦¬"]
    
    # ì§ˆë¬¸ì— ì·¨ì—… ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    if any(keyword in query for keyword in job_keywords):
        if jobs_db:
            print(f"[ğŸ” DB ë¼ìš°íŒ…] '{query}' -> ì·¨ì—… ì •ë³´ DB ì„ íƒ")
            return jobs_db
        else:
            print("[ğŸ” DB ë¼ìš°íŒ…] ì·¨ì—… ì •ë³´ DBê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ê³µì§€ì‚¬í•­ DB ì‚¬ìš©")
            return notices_db # ì·¨ì—… DBê°€ ì—†ìœ¼ë©´ ê³µì§€ì‚¬í•­ DBë¼ë„ ì‚¬ìš©
    else:
        if notices_db:
            print(f"[ğŸ” DB ë¼ìš°íŒ…] '{query}' -> ê³µì§€ì‚¬í•­ DB ì„ íƒ")
            return notices_db
        else:
            print("[ğŸ” DB ë¼ìš°íŒ…] ê³µì§€ì‚¬í•­ DBê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ì·¨ì—… ì •ë³´ DB ì‚¬ìš©")
            return jobs_db # ê³µì§€ì‚¬í•­ DBê°€ ì—†ìœ¼ë©´ ì·¨ì—… DBë¼ë„ ì‚¬ìš©


# --- 4. ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜ ---
def search_similar_documents(query: str, top_k: int = 5):
    """
    ë¼ìš°íŒ…ëœ Vector DBì—ì„œ queryì™€ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬,
    LangChain RAG ì²´ì¸ì— ì í•©í•œ í˜•íƒœë¡œ contextë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # 4-1. ë¼ìš°í„°ë¥¼ í†µí•´ ì–´ë–¤ DBë¥¼ ê²€ìƒ‰í• ì§€ ê²°ì •í•©ë‹ˆë‹¤.
    selected_db = route_query_to_db(query)
    
    if not selected_db:
        # ë‘ DB ëª¨ë‘ ë¡œë”©ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°
        print("ê²½ê³ : ë¡œë“œëœ Vector DBê°€ ì—†ìŠµë‹ˆë‹¤.")
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (Vector DB ë¡œë”© ì‹¤íŒ¨).", []

    # 4-2. ì„ íƒëœ DBì—ì„œë§Œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    results: list[Document] = selected_db.similarity_search(query, k=top_k)
    
    # 4-3. RAG í”„ë¡¬í”„íŠ¸ì— ë“¤ì–´ê°ˆ context ë¬¸ìì—´ê³¼ field_namesë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    context = ""
    field_names = set()
    for doc in results:
        # Document ê°ì²´ì˜ page_content ì†ì„±ì— ë³¸ë¬¸ì´, metadata ì†ì„±ì— ì¶œì²˜ ì •ë³´ê°€ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.
        context += f"- ë¬¸ì„œ ì œëª©: {doc.metadata.get('title', 'ì œëª© ì—†ìŒ')}\n"
        context += f"  - ì¶œì²˜: {doc.metadata.get('source', 'ì¶œì²˜ ì—†ìŒ')}\n"
        context += f"  - ë‚´ìš©: {doc.page_content}\n---\n"
        field_names.update(doc.metadata.keys()) # ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° í‚¤ë¥¼ ìˆ˜ì§‘

    return context, list(field_names)


# --------------------------------------------------------------------------
# ì°¸ê³ : LLMì„ ì´ìš©í•œ ìš”ì•½/ì¬ì •ë ¬ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
# ì´ í•¨ìˆ˜ëŠ” í˜„ì¬ ë©”ì¸ RAG ì²´ì¸ì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, í•„ìš”ì‹œ ë³„ë„ë¡œ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìœ ìš©í•œ ìœ í‹¸ë¦¬í‹°ì…ë‹ˆë‹¤.
# ì‚¬ìš©í•˜ë ¤ë©´ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.
# import openai
# from tenacity import retry, stop_after_attempt, wait_exponential
# client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
# def get_llm_summary(query, results):
#     """ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMì´ ë§¥ë½ì„ ê³ ë ¤í•´ ìš”ì•½í•˜ê³  ì¬ì •ë ¬í•©ë‹ˆë‹¤."""
#     # ... (ê¸°ì¡´ ìš”ì•½ í•¨ìˆ˜ ì½”ë“œ)
# --------------------------------------------------------------------------