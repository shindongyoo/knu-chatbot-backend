# app/search_engine.py
import os
import re
import faiss
import pickle
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_community.docstore.in_memory import InMemoryDocstore
from app.database import chatbot_db
from langchain.tools import tool # <-- [ìƒˆë¡œ ì¶”ê°€] AI ë„êµ¬ import

load_dotenv()

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",  # <-- "ada-002"ì—ì„œ ë³€ê²½!
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

# app/search_engine.py ì˜ load_vector_db_manually í•¨ìˆ˜ë¥¼ ì´ê±¸ë¡œ êµì²´

def load_vector_db_manually(folder_path, index_name):
    faiss_path = os.path.join(folder_path, f"{index_name}.faiss")
    pkl_path = os.path.join(folder_path, f"{index_name}.pkl") # ì´ì œ ì´ê²Œ 'ì¢‹ì€ ì£¼ì†Œë¡'
    if not os.path.exists(faiss_path) or not os.path.exists(pkl_path):
        raise FileNotFoundError(f"'{folder_path}'ì—ì„œ DB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_name}")
    
    index = faiss.read_index(faiss_path)
    with open(pkl_path, "rb") as f:
        # docs_dataëŠ” ì´ì œ [{'id':..., 'title':..., 'content':..., 'url':...}, ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
        docs_data = pickle.load(f) 
        
    documents = []
    docstore_dict = {}
    index_to_docstore_id = {}

    # â–¼â–¼â–¼ [í•µì‹¬ ìˆ˜ì •: Document ìƒì„± ë°©ì‹ ë³€ê²½] â–¼â–¼â–¼
    for i, doc_dict in enumerate(docs_data):
        # DB ìƒì„± ì‹œ ì‚¬ìš©ëœ 'full_text'ì™€ ìœ ì‚¬í•˜ê²Œ page_contentë¥¼ ì¬êµ¬ì„±
        # (DB ìƒì„± ì½”ë“œì˜ metadata í¬ë§·ì„ ì°¸ê³ í•˜ì—¬ í•„ë“œ ì¶”ê°€/ìˆ˜ì • í•„ìš”)
        metadata_str = (
            f"ğŸ“Œ ì œëª©: {doc_dict.get('title', '').strip()}\n"
            f"ğŸ“… ì‘ì„±ì¼: {doc_dict.get('date', '').strip()}\n"
            f"ğŸ¢ ê¸°ì—…ëª…: {doc_dict.get('company', 'N/A')}\n"
        )
        content_chunk = doc_dict.get('content', '').strip()
        detail_url = doc_dict.get('url', '') # 'url' í‚¤ ì‚¬ìš© (DB ìƒì„± ì½”ë“œ ì°¸ê³ )

        # DB ìƒì„± ì½”ë“œì˜ full_text í¬ë§·ê³¼ ìµœëŒ€í•œ ìœ ì‚¬í•˜ê²Œ ë§Œë“¦
        reconstructed_page_content = f"{metadata_str}\n{content_chunk}\n\nğŸ”— ìì„¸í•œ ë‚´ìš©ì€ ë§í¬ë¥¼ ì°¸ê³ í•˜ì„¸ìš”: {detail_url}"
        
        # ë©”íƒ€ë°ì´í„°ì—ëŠ” ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ì „ì²´ë¥¼ ë„£ì–´ë„ ë˜ê³ , í•„ìš”í•œ ê²ƒë§Œ ë„£ì–´ë„ ë¨
        metadata = doc_dict.copy() # ì›ë³¸ ë³µì‚¬í•´ì„œ ì‚¬ìš©

        # LangChain Document ê°ì²´ ìƒì„± (page_contentì— ì¬êµ¬ì„±ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©)
        doc_obj = Document(page_content=reconstructed_page_content, metadata=metadata)
        documents.append(doc_obj)
        
        # Docstore ë° ë§¤í•‘ ìƒì„± (ê¸°ì¡´ ë¡œì§)
        doc_id = str(i)
        docstore_dict[doc_id] = doc_obj
        index_to_docstore_id[i] = doc_id
    # â–²â–²â–² [ìˆ˜ì • ì™„ë£Œ] â–²â–²â–²

    docstore = InMemoryDocstore(docstore_dict)

    # LangChainFAISS ê°ì²´ ìƒì„± (embedding_function ì‚¬ìš©)
    return LangChainFAISS(
        embedding_function=embeddings, 
        index=index, 
        docstore=docstore, 
        index_to_docstore_id=index_to_docstore_id
    )

# Vector DB ë¡œë”©
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
notices_title_db = None
notices_content_db = None
jobs_db = None

try:
    NOTICES_DB_DIR = os.path.join(BASE_DIR, '..', 'vector_store', 'notices')
    notices_title_db = load_vector_db_manually(NOTICES_DB_DIR, "notices_title_index")
    print("âœ… Notices (ì œëª©) Vector DB ë¡œë”© ì„±ê³µ.")
except Exception as e:
    print(f"âŒ Notices (ì œëª©) Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")

try:
    NOTICES_DB_DIR = os.path.join(BASE_DIR, '..', 'vector_store', 'notices')
    notices_content_db = load_vector_db_manually(NOTICES_DB_DIR, "notices_content_index")
    print("âœ… Notices (ë³¸ë¬¸) Vector DB ë¡œë”© ì„±ê³µ.")
except Exception as e:
    print(f"âŒ Notices (ë³¸ë¬¸) Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")

try:
    JOBS_DB_DIR = os.path.join(BASE_DIR, '..', 'vector_store', 'jobs')
    jobs_db = load_vector_db_manually(JOBS_DB_DIR, "jobs_openai_index")
    print("âœ… Jobs Vector DB ë¡œë”© ì„±ê³µ.")
except Exception as e:
    print(f"âŒ Jobs Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")


# --- 2. MongoDBì—ì„œ êµ¬ì„±ì› ì •ë³´ ê²€ìƒ‰ í•¨ìˆ˜ ---

def search_members_in_mongodb(query: str):
    match = re.search(r'([\wê°€-í£]{2,4})\s*(êµìˆ˜ë‹˜|êµìˆ˜|ì¡°êµ|ì„ ìƒë‹˜)', query)
    if not match:
        return None

    name_to_search = match.group(1)
    members = list(chatbot_db.members.find({"name": {"$regex": name_to_search}}))
    
    if members:
        context = "### ê²€ìƒ‰ëœ êµ¬ì„±ì› ì •ë³´:\n"
        for member in members:
            context += f"- ì´ë¦„: {member.get('name', 'ì •ë³´ ì—†ìŒ')}\n"
            context += f"  - ì§ìœ„: {member.get('position', 'ì •ë³´ ì—†ìŒ')}\n"
            context += f"  - ì—°êµ¬ì‹¤: {member.get('lab', 'ì •ë³´ ì—†ìŒ')}\n"
            context += f"  - ì´ë©”ì¼: {member.get('email', 'ì •ë³´ ì—†ìŒ')}\n"
            context += f"  - ì „í™”ë²ˆí˜¸: {member.get('phone', 'ì •ë³´ ì—†ìŒ')}\n---\n"
        return context
    return None

# --- 3. ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜ (ë¼ìš°í„° ë¡œì§ í†µí•©) ---
@tool
def search_similar_documents(query: str, top_k: int = 3) -> str:
    """
    "ìˆ˜ê°•ì‹ ì²­", "ì¥í•™ìƒ", "ì·¨ì—… ì •ë³´", "êµìˆ˜ë‹˜ ì •ë³´" ë“± 
    'ì¡¸ì—… ìš”ê±´'ì„ ì œì™¸í•œ ëª¨ë“  ì¼ë°˜ì ì¸ êµë‚´ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    (ì˜ˆ: "ì¥í•™ìƒ ê´€ë ¨ì •ë³´ ì•Œë ¤ì¤˜", "í•œì„¸ê²½ êµìˆ˜ë‹˜ ì´ë©”ì¼ ì•Œë ¤ì¤˜")
    """
    print(f"\n--- [ì—ì´ì „íŠ¸ ë„êµ¬ 1: ì¼ë°˜ ê²€ìƒ‰] '{query}' ê²€ìƒ‰ ì‹œì‘ ---")
    member_keywords = ["êµìˆ˜", "êµìˆ˜ë‹˜", "ì—°êµ¬ì‹¤", "ì´ë©”ì¼", "ì—°ë½ì²˜", "ì¡°êµ", "ì„ ìƒë‹˜"]
    job_keywords = ["ì·¨ì—…", "ì¸í„´", "ì±„ìš©", "íšŒì‚¬", "ì§ë¬´"]

    # (MongoDB ë¼ìš°íŒ… ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
    if any(keyword in query for keyword in member_keywords):
        print(f"[ğŸ” DB ë¼ìš°íŒ…] '{query}' -> MongoDB ê²€ìƒ‰ ì‹œë„")
        mongo_context = search_members_in_mongodb(query)
        if mongo_context:
            return mongo_context
        else:
            print(f"[ğŸ” DB ë¼ìš°íŒ…] MongoDB ê²°ê³¼ ì—†ìŒ. Vector DBë¡œ ê³„ì† ì§„í–‰...")
    
    # (Vector DB ê²€ìƒ‰ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
    selected_dbs = None
    if any(keyword in query for keyword in job_keywords):
        selected_dbs = (jobs_db,)
    else:
        selected_dbs = (notices_title_db, notices_content_db)
    
    if not any(db for db in selected_dbs if db is not None):
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (DB ë¡œë”© ì‹¤íŒ¨)."

    all_results_with_scores = []
    for db in selected_dbs:
        if db:
            results = db.similarity_search_with_score(query, k=top_k)
            all_results_with_scores.extend(results)

    # (ì¤‘ë³µ ì œê±° ë° ì •ë ¬ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
    unique_results = {}
    for doc, score in all_results_with_scores:
        if doc.page_content not in unique_results or score < unique_results[doc.page_content][1]:
            unique_results[doc.page_content] = (doc, score)
    sorted_results = sorted(unique_results.values(), key=lambda item: item[1])

    # (Context ìƒì„± ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
    context = ""
    for doc, score in sorted_results[:top_k]:
        context += f"- ë‚´ìš© (ì ìˆ˜: {score:.4f}): {doc.page_content}\n---\n"

    if not context:
        return "ê²€ìƒ‰ëœ ì°¸ê³  ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤."
    else:
        return context

@tool
def get_graduation_info(student_id_prefix: str, abeek_bool: bool) -> str:
    """
    'ì¡¸ì—… ìš”ê±´'ì— ëŒ€í•œ ì§ˆë¬¸ì— "ìµœì¢…ì ìœ¼ë¡œ" ë‹µí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. 
    ë°˜ë“œì‹œ ì‚¬ìš©ìì˜ 'student_id_prefix'(ì˜ˆ: "20")ì™€ 'abeek_bool'(True/False)ì„ 
    ë¨¼ì € ì•Œì•„ë‚¸ í›„ì—ë§Œ ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
    """
    print(f"\n--- [ì—ì´ì „íŠ¸ ë„êµ¬ 2: ì¡¸ì—…ìš”ê±´ ê²€ìƒ‰] í•™ë²ˆ: {student_id_prefix}, ABEEK: {abeek_bool} ---")
    try:
        collection = chatbot_db["graduation_requirements"] 
        
        # (í•™ë²ˆ ë³€í™˜ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
        search_year = -1 
        try:
            year_prefix_num = int(student_id_prefix)
            if 0 <= year_prefix_num <= 99: 
                search_year = 2000 + year_prefix_num 
            else:
                search_year = year_prefix_num
        except ValueError:
            return f"ì…ë ¥í•˜ì‹  í•™ë²ˆ '{student_id_prefix}'ì´(ê°€) ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."
        
        # (í•™ë²ˆ ë²”ìœ„ ê²€ìƒ‰ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
        query = { "abeek": abeek_bool }
        all_reqs_for_abeek = list(collection.find(query))
        result = None 
        
        for req_doc in all_reqs_for_abeek:
            range_str = req_doc.get("applied_year_range", "") 
            start_year, end_year = -1, float('inf') 
            try:
                year_numbers = re.findall(r'\d+', range_str)
                if len(year_numbers) == 1: 
                    range_start_year = int(year_numbers[0])
                elif len(year_numbers) == 2: 
                    range_start_year = int(year_numbers[0])
                    range_end_year = int(year_numbers[1])
                
                is_match = (range_start_year <= search_year) and (search_year <= range_end_year)
                
                if is_match:
                    result = req_doc
                    break 
            except Exception:
                continue
            
        # --- [3. Context ìƒì„± (ìµœì¢… ìƒì„¸ ìŠ¤í‚¤ë§ˆ ë°˜ì˜)] ---
        if result:
            # â–¼â–¼â–¼ [í•µì‹¬ ìˆ˜ì •: ìƒì„¸ ìŠ¤í‚¤ë§ˆ ë°˜ì˜] â–¼â–¼â–¼
            
            # ì•ˆì „í•˜ê²Œ ë°ì´í„° ì¶”ì¶œ (ê°ì²´ê°€ ì—†ìœ¼ë©´ ë¹ˆ dict ë°˜í™˜)
            requirements = result.get('requirements', {}) 
            credits = requirements.get('credits', {})
            credit_basic = credits.get('ê¸°ë³¸ì†Œì–‘', {}) 
            credit_msc = credits.get('ì „ê³µê¸°ë°˜', "N/A") # ì „ê³µê¸°ë°˜ì€ ê°ì²´ê°€ ì•„ë‹Œ ì§ì ‘ ê°’
            credit_major = credits.get('ê³µí•™ì „ê³µ', {})
            
            courses = requirements.get('required_courses', {}) 
            courses_basic = courses.get('ê¸°ë³¸ì†Œì–‘', [])
            courses_msc = courses.get('ì „ê³µê¸°ë°˜', [])
            courses_major = courses.get('ê³µí•™ì „ê³µ', [])

            english = requirements.get('english', {})
            eng_tests = english.get('tests', [])
            eng_sub = english.get('substitution', [])
            eng_notes = english.get('notes', [])

            grad_qual = requirements.get('graduation_qualification', {})
            advisor = grad_qual.get('advisor_consultation', {})
            software = grad_qual.get('software_credits', {})
            sw_courses = software.get('required_courses', [])
            sw_sub = software.get('substitution', [])
            grad_qual_notes = grad_qual.get('notes', [])

            # ë¦¬ìŠ¤íŠ¸(Array) ì •ë³´ë¥¼ ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³€í™˜
            def format_list(items):
                return ", ".join(items) if items else "í•´ë‹¹ ì—†ìŒ"
            
            # ì˜ì–´ ì‹œí—˜ ìš”ê±´ í¬ë§·íŒ…
            def format_eng_tests(tests):
                if not tests: return "í•´ë‹¹ ì—†ìŒ"
                return ", ".join([f"{test.get('name')}: {test.get('score')}ì  ì´ìƒ" for test in tests])

            context = f"""
            [ê²€ìƒ‰ëœ ë§ì¶¤í˜• ì¡¸ì—… ìš”ê±´ ({student_id_prefix}í•™ë²ˆ, ABEEK {'O' if abeek_bool else 'X'})] 
            - ì ìš© í•™ë²ˆ(DB): {result.get('applied_year_range', 'N/A')} ê¸°ì¤€

            [1. í•™ì  ìš”ê±´ (Credits)]
            - ì´ ì´ìˆ˜ í•™ì : {credits.get('total', 'N/A')}í•™ì 
            - ê¸°ë³¸ì†Œì–‘(êµì–‘): {credit_basic.get('min', 'N/A')}í•™ì  ì´ìƒ
            - ì „ê³µê¸°ë°˜(MSC): {credit_msc}í•™ì 
            - ê³µí•™ì „ê³µ(ì „ê³µ): {credit_major.get('total', 'N/A')}í•™ì  (ì´ ì¤‘ ì„¤ê³„ {credit_major.get('design', 'N/A')}í•™ì  í¬í•¨)
            - ì „ê³µ ì°¸ê³ : {credit_major.get('note', 'N/A')}

            [2. í•„ìˆ˜ ê³¼ëª© ìš”ê±´ (Required Courses)] 
            - ê¸°ë³¸ì†Œì–‘ í•„ìˆ˜: {format_list(courses_basic)}
            - ì „ê³µê¸°ë°˜ í•„ìˆ˜: {format_list(courses_msc)}
            - ê³µí•™ì „ê³µ í•„ìˆ˜: {format_list(courses_major)}

            [3. ì˜ì–´ ìš”ê±´ (English)]
            - ê³µì¸ ì‹œí—˜ ê¸°ì¤€: {format_eng_tests(eng_tests)}
            - ë©´ì œ ê¸°ì¤€: {format_list(eng_sub)}
            - ë¹„ê³ : {format_list(eng_notes)}

            [4. ì¡¸ì—… ìê²© (Graduation Qualification)]
            - ì§€ë„êµìˆ˜ ìƒë‹´: {advisor.get('count', 'N/A')}íšŒ ì´ìƒ ({advisor.get('note', 'N/A')})
            - ì†Œí”„íŠ¸ì›¨ì–´ ì´ìˆ˜: {software.get('min', 'N/A')}í•™ì  ì´ìƒ ({software.get('note', 'N/A')})
            - (SW ì¸ì • ê³¼ëª©: {format_list(sw_courses)})
            - (SW ë©´ì œ ê¸°ì¤€: {format_list(sw_sub)})
            - ì¡¸ì—… ìê²© ë¹„ê³ : {format_list(grad_qual_notes)}

            [ì¢…í•© ë¹„ê³ ]
            {format_list(requirements.get('notes', []))}
            """
            # â–²â–²â–² [ìˆ˜ì • ì™„ë£Œ] â–²â–²â–²
            return context
        else:
            fail_msg = f"{student_id_prefix}í•™ë²ˆ, ABEEK {'O' if abeek_bool else 'X'} í•™ìƒì— ëŒ€í•œ ë§ì¶¤í˜• ì¡¸ì—… ìš”ê±´ì„ DBì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            return fail_msg
            
    except Exception as e:
        print(f"!!!!!!!!!!!!!! MongoDB ì¡¸ì—… ìš”ê±´ ê²€ìƒ‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ !!!!!!!!!!!!!!")
        import traceback
        traceback.print_exc()
        return "ì¡¸ì—… ìš”ê±´ DBë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."