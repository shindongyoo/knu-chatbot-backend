# app/search_engine.py (ë‘ DB ëª¨ë‘ ê²€ìƒ‰í•˜ëŠ” ìµœì¢… ë²„ì „)

import os
import pickle
import faiss
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_community.docstore.in_memory import InMemoryDocstore

load_dotenv()

# --- 1. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ---
embeddings = OpenAIEmbeddings(
    model="text-embedding-ada-002",
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

def load_vector_db_manually(folder_path, index_name):
    faiss_path = os.path.join(folder_path, f"{index_name}.faiss")
    pkl_path = os.path.join(folder_path, f"{index_name}.pkl")
    if not os.path.exists(faiss_path) or not os.path.exists(pkl_path):
        raise FileNotFoundError(f"'{folder_path}'ì—ì„œ DB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_name}")
    index = faiss.read_index(faiss_path)
    with open(pkl_path, "rb") as f:
        docs_data = pickle.load(f)
    documents = [Document(page_content=doc.pop('content', ''), metadata=doc) for doc in docs_data]
    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})
    index_to_docstore_id = {i: str(i) for i in range(len(documents))}
    return LangChainFAISS(
        embedding_function=embeddings,
        index=index,
        docstore=docstore,
        index_to_docstore_id=index_to_docstore_id
    )

# --- 2. ëª¨ë“  Vector DBë¥¼ ë¡œë”©í•©ë‹ˆë‹¤ ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
notices_title_db = None
notices_content_db = None # ë³¸ë¬¸ DBë¥¼ ìœ„í•œ ìƒˆ ë³€ìˆ˜
jobs_db = None
members_db = None

# ê³µì§€ì‚¬í•­ 'ì œëª©' DB ë¡œë”©
try:
    NOTICES_DB_DIR = os.path.join(BASE_DIR, '..', 'vector_store', 'notices')
    notices_title_db = load_vector_db_manually(NOTICES_DB_DIR, "notices_title_index")
    print("âœ… Notices (ì œëª©) Vector DB ë¡œë”© ì„±ê³µ.")
except Exception as e:
    print(f"âŒ Notices (ì œëª©) Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")

# ê³µì§€ì‚¬í•­ 'ë³¸ë¬¸' DB ë¡œë”© (ìƒˆë¡œ ì¶”ê°€)
try:
    NOTICES_DB_DIR = os.path.join(BASE_DIR, '..', 'vector_store', 'notices')
    # notices_content_index.faiss ì™€ notices_content_index.pkl íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.
    # ì´ì „ ë‹¨ê³„ì—ì„œ notices.index -> notices_content_index.faiss
    # notices_metadata.pkl -> notices_content_index.pkl ë¡œ ì´ë¦„ì„ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
    notices_content_db = load_vector_db_manually(NOTICES_DB_DIR, "notices_content_index")
    print("âœ… Notices (ë³¸ë¬¸) Vector DB ë¡œë”© ì„±ê³µ.")
except Exception as e:
    print(f"âŒ Notices (ë³¸ë¬¸) Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")

# ì·¨ì—… ì •ë³´ DB ë¡œë”©
try:
    JOBS_DB_DIR = os.path.join(BASE_DIR, '..', 'vector_store', 'jobs')
    jobs_db = load_vector_db_manually(JOBS_DB_DIR, "jobs_openai_index")
    print("âœ… Jobs Vector DB ë¡œë”© ì„±ê³µ.")
except Exception as e:
    print(f"âŒ Jobs Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")
    
try:
    MEMBERS_DB_DIR = os.path.join(BASE_DIR, '..', 'vector_store', 'members')
    members_db = load_vector_db_manually(MEMBERS_DB_DIR, "members_index")
    print("âœ… Members Vector DB ë¡œë”© ì„±ê³µ.")
except Exception as e:
    print(f"âŒ Members Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")


# --- 3. ë¼ìš°í„° í•¨ìˆ˜ ìˆ˜ì • ---
def route_query_to_db(query: str):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ Vector DB(ë“¤)ì„ ì„ íƒí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # 1. ê° ë°ì´í„°ë² ì´ìŠ¤ì˜ ì—­í• ì„ ëª…í™•íˆ í•˜ëŠ” í‚¤ì›Œë“œ ëª©ë¡ ì •ì˜
    
    # members_db í‚¤ì›Œë“œ: ì¸ë¬¼, ì¥ì†Œ, ì—°ë½ì²˜ ë“± ê³ ìœ ëª…ì‚¬ ì •ë³´
    member_keywords = [
        "êµìˆ˜", "êµìˆ˜ë‹˜", "ì¡°êµ", "êµì§ì›", "ì—°êµ¬ì‹¤", "ì‚¬ë¬´ì‹¤", "ì´ë©”ì¼", 
        "ì—°ë½ì²˜", "ì „í™”ë²ˆí˜¸", "ìœ„ì¹˜", "ì–´ë””", "í˜¸ê´€", "í˜¸ì‹¤", "ê°•ì˜ì‹¤"
    ]
    
    # jobs_db í‚¤ì›Œë“œ: ì±„ìš©, ê²½ë ¥ ê´€ë ¨ ì •ë³´
    job_keywords = [
        "ì·¨ì—…", "ì¸í„´", "ì±„ìš©", "íšŒì‚¬", "ì§ë¬´", "ìì†Œì„œ", "ë©´ì ‘", 
        "êµ¬ì¸", "ëª¨ì§‘", "ê³µê³ ", "ì¼ìë¦¬", "ì»¤ë¦¬ì–´", "ê²½ë ¥"
    ]

    # notices_db í‚¤ì›Œë“œ: í•™ì‚¬, í–‰ì •, ì¥í•™ê¸ˆ ë“± ì¼ë°˜ ì •ë³´
    notice_keywords = [
        "ê³µì§€", "ì¥í•™ê¸ˆ", "ë“±ë¡ê¸ˆ", "ì‹ ì²­", "ê¸°ê°„", "ì œì¶œ", "ë§ˆê°", "ì•ˆë‚´",
        "ì¡¸ì—…", "ìš”ê±´", "í•™ì ", "ìˆ˜ê°•", "ê³¼ëª©", "êµê³¼", "ê³¼ì •", "ì´ìˆ˜", "ìš”ëŒ"
    ]

    # 2. ë¼ìš°íŒ… ë¡œì§: ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ë¶€í„° í™•ì¸ (êµìˆ˜/ì¥ì†Œ -> ì·¨ì—… -> ì¼ë°˜ê³µì§€)

    # 1ìˆœìœ„: êµ¬ì„±ì› ë˜ëŠ” ì¥ì†Œ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸
    if any(keyword in query for keyword in member_keywords):
        print(f"[ğŸ” DB ë¼ìš°íŒ…] '{query}' -> êµ¬ì„±ì›/ì¥ì†Œ ì •ë³´ DB ì„ íƒ")
        return (members_db,) if members_db else (notices_content_db,)

    # 2ìˆœìœ„: ì·¨ì—… ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸
    if any(keyword in query for keyword in job_keywords):
        print(f"[ğŸ” DB ë¼ìš°íŒ…] '{query}' -> ì·¨ì—… ì •ë³´ DB ì„ íƒ")
        return (jobs_db,) if jobs_db else (notices_content_db,)
    
    # 3ìˆœìœ„: ì¼ë°˜ ê³µì§€ ë° í•™ì‚¬ ê´€ë ¨ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ DB ê²€ìƒ‰ (ì œëª©+ë³¸ë¬¸)
    # (ìœ„ ë‘ ê²½ìš°ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ëª¨ë“  ì§ˆë¬¸ì€ ì—¬ê¸°ë¡œ ì˜µë‹ˆë‹¤)
    print(f"[ğŸ” DB ë¼ìš°íŒ…] '{query}' -> ê³µì§€ì‚¬í•­ DB ì„ íƒ (ì œëª©+ë³¸ë¬¸)")
    return (notices_title_db, notices_content_db)

# --- 4. ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜ ìˆ˜ì • ---
def search_similar_documents(query: str, top_k: int = 5):
    # 4-1. ë¼ìš°í„°ë¥¼ í†µí•´ ê²€ìƒ‰í•  DB(ë“¤)ì„ ê²°ì •í•©ë‹ˆë‹¤.
    selected_dbs = route_query_to_db(query)
    
    if not any(selected_dbs):
        print("ê²½ê³ : ë¡œë“œëœ Vector DBê°€ ì—†ìŠµë‹ˆë‹¤.")
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (Vector DB ë¡œë”© ì‹¤íŒ¨).", []

    # 4-2. ì„ íƒëœ ëª¨ë“  DBì—ì„œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ í•©ì¹©ë‹ˆë‹¤.
    all_results = []
    for db in selected_dbs:
        if db:
            results = db.similarity_search_with_score(query, k=top_k)
            all_results.extend(results)
    
    # 4-3. ì¤‘ë³µì„ ì œê±°í•˜ê³  ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
    unique_results = {}
    for doc, score in all_results:
        # page_contentë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤. ë™ì¼ ë¬¸ì„œë©´ ì ìˆ˜ê°€ ë” ì¢‹ì€(ë‚®ì€) ê²ƒìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
        if doc.page_content not in unique_results or score < unique_results[doc.page_content][1]:
            unique_results[doc.page_content] = (doc, score)
            
    # ì ìˆ˜ê°€ ë‚®ì€ ìˆœ (ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœ)ìœ¼ë¡œ ì •ë ¬
    sorted_results = sorted(unique_results.values(), key=lambda item: item[1])
    
    # 4-4. ìµœì¢… contextë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    context = ""
    field_names = set()
    for doc, score in sorted_results[:top_k]: # ìµœì¢…ì ìœ¼ë¡œ top_kê°œë§Œ ì‚¬ìš©
        context += f"ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}\n"
        context += f"ë¬¸ì„œ ì œëª©: {doc.metadata.get('title', 'ì œëª© ì—†ìŒ')}\n"
        context += f"ì¶œì²˜: {doc.metadata.get('source', 'ì¶œì²˜ ì—†ìŒ')}\n"
        context += f"ë‚´ìš©: {doc.page_content}\n---\n"
        field_names.update(doc.metadata.keys())
        
    return context, list(field_names)