import os
import re
import faiss
import pickle
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_community.docstore.in_memory import InMemoryDocstore
from app.database import chatbot_db

load_dotenv()

# ì„ë² ë”© ëª¨ë¸ ì„¤ì • (DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•˜ê²Œ 'text-embedding-3-small'ë¡œ ì„¤ì •)
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small", 
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

def load_vector_db_manually(folder_path, index_name):
    """ì§€ì •ëœ ê²½ë¡œì—ì„œ FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë¬¸ì„œ êµ¬ì¡°ë¥¼ ë§ì¶¥ë‹ˆë‹¤."""
    faiss_path = os.path.join(folder_path, f"{index_name}.faiss")
    pkl_path = os.path.join(folder_path, f"{index_name}.pkl")
    if not os.path.exists(faiss_path) or not os.path.exists(pkl_path):
        raise FileNotFoundError(f"'{folder_path}'ì—ì„œ DB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_name}")
    
    index = faiss.read_index(faiss_path)
    with open(pkl_path, "rb") as f:
        docs_data = pickle.load(f)
        
    # â˜…â˜…â˜… ìˆ˜ì • 1: ë©”íƒ€ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ ë¬¸ì„œ ë‚´ìš©(page_content) ì¶”ì¶œ ë°©ì‹ì„ ìˆ˜ì •í•©ë‹ˆë‹¤. â˜…â˜…â˜…
    # 'content' í•„ë“œê°€ ì—†ìœ¼ë©´ 'table_content'ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    documents = [
        Document(
            page_content=doc.pop('content', doc.get('table_content', '')), 
            metadata=doc
        ) 
        for doc in docs_data
    ]
    
    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})
    index_to_docstore_id = {i: str(i) for i in range(len(documents))}
    
    return LangChainFAISS(embedding_function=embeddings, index=index, docstore=docstore, index_to_docstore_id=index_to_docstore_id)

# Vector DB ë¡œë”©
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# â˜…â˜…â˜… ìˆ˜ì • 2: notices_title_db ë³€ìˆ˜ ì„ ì–¸ ì œê±° (ë‹¨ì¼ DB ì‚¬ìš©) â˜…â˜…â˜…
notices_content_db = None
jobs_db = None

# â˜…â˜…â˜… ìˆ˜ì • 3: notices_title_db ë¡œë”© ë¸”ë¡ ì „ì²´ ì œê±° (ë‹¨ì¼ DB ì‚¬ìš©) â˜…â˜…â˜…
# try:
#     NOTICES_DB_DIR = os.path.join(BASE_DIR, '..', 'vector_store', 'notices')
#     notices_title_db = load_vector_db_manually(NOTICES_DB_DIR, "notices_title_index")
#     print("âœ… Notices (ì œëª©) Vector DB ë¡œë”© ì„±ê³µ.")
# except Exception as e:
#     print(f"âŒ Notices (ì œëª©) Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")

try:
    NOTICES_DB_DIR = os.path.join(BASE_DIR, '..', 'vector_store', 'notices')
    # ê³µì§€ì‚¬í•­ DB íŒŒì¼ëª…ì„ ì„ë² ë”© ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•˜ê²Œ ì§€ì •í•©ë‹ˆë‹¤.
    notices_content_db = load_vector_db_manually(NOTICES_DB_DIR, "notices_content_index")
    print("âœ… Notices Vector DB ë¡œë”© ì„±ê³µ (notices_content_index).")
except Exception as e:
    print(f"âŒ Notices Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")

try:
    JOBS_DB_DIR = os.path.join(BASE_DIR, '..', 'vector_store', 'jobs')
    jobs_db = load_vector_db_manually(JOBS_DB_DIR, "jobs_openai_index")
    print("âœ… Jobs Vector DB ë¡œë”© ì„±ê³µ.")
except Exception as e:
    print(f"âŒ Jobs Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")


# --- 2. MongoDBì—ì„œ êµ¬ì„±ì› ì •ë³´ ê²€ìƒ‰ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---

def search_members_in_mongodb(query: str):
    match = re.search(r'([\wê°€-í£]{2,4})\s*(êµìˆ˜ë‹˜|êµìˆ˜|ì¡°êµ|ì„ ìƒë‹˜)', query)
    if not match:
        return None

    name_to_search = match.group(1)
    members = list(chatbot_db.members.find({"name": {"$regex": name_to_search}}))
    
    if members:
        context = "### ê²€ìƒ‰ëœ êµ¬ì„±ì› ì •ë³´:\n"
        for member in members:
            context += f"- ì´ë¦„: {member.get('name', 'ì •ë³´ ì—†ìŒ')}\n"
            context += f"  - ì§ìœ„: {member.get('position', 'ì •ë³´ ì—†ìŒ')}\n"
            context += f"  - ì—°êµ¬ì‹¤: {member.get('lab', 'ì •ë³´ ì—†ìŒ')}\n"
            context += f"  - ì´ë©”ì¼: {member.get('email', 'ì •ë³´ ì—†ìŒ')}\n"
            context += f"  - ì „í™”ë²ˆí˜¸: {member.get('phone', 'ì •ë³´ ì—†ìŒ')}\n---\n"
        return context
    return None

# --- 3. ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜ (ë¼ìš°í„° ë¡œì§ í†µí•©) ---

def search_similar_documents(query: str, top_k: int = 1):
    print(f"--- [ì§„ë‹¨ 1/5] '{query}'ì— ëŒ€í•œ ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘ (top_k={top_k}) ---")
    member_keywords = ["êµìˆ˜", "êµìˆ˜ë‹˜", "ì—°êµ¬ì‹¤", "ì´ë©”ì¼", "ì—°ë½ì²˜", "ì¡°êµ", "ì„ ìƒë‹˜", "ì‚¬ë¬´ì‹¤", "ìœ„ì¹˜", "í˜¸ê´€", "í˜¸ì‹¤"]
    job_keywords = ["ì·¨ì—…", "ì¸í„´", "ì±„ìš©", "íšŒì‚¬", "ì§ë¬´", "ìì†Œì„œ", "ë©´ì ‘", "ê³µê³ "]

    if any(keyword in query for keyword in member_keywords):
        print(f"[ğŸ” DB ë¼ìš°íŒ…] '{query}' -> MongoDB êµ¬ì„±ì› ê²€ìƒ‰ ì‹œë„")
        mongo_context = search_members_in_mongodb(query)
        
        if mongo_context:
            print(f"--- [ì§„ë‹¨ 5/5] MongoDBì—ì„œ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ. ---")
            return mongo_context, ['name', 'position', 'lab', 'email', 'phone']
        
    selected_dbs = None
    if any(keyword in query for keyword in job_keywords):
        print("[ì§„ë‹¨] ì·¨ì—… DBë¥¼ ì„ íƒí•©ë‹ˆë‹¤.")
        selected_dbs = (jobs_db,)
    else:
        print("[ì§„ë‹¨] ê³µì§€ì‚¬í•­ DBë¥¼ ì„ íƒí•©ë‹ˆë‹¤.")
        # â˜…â˜…â˜… ìˆ˜ì • 4: notices_content_db í•˜ë‚˜ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤. â˜…â˜…â˜…
        selected_dbs = (notices_content_db,)
        
    if not any(db for db in selected_dbs if db is not None):
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (DB ë¡œë”© ì‹¤íŒ¨).", []

    all_results = []
    for db in selected_dbs:
        if db:
            print(f"--- [ì§„ë‹¨ 2/5] DB ê°ì²´ì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤í–‰ ---")
            # top_këŠ” main.pyì—ì„œ ì „ë‹¬ë˜ëŠ” ê°’ì…ë‹ˆë‹¤.
            results = db.similarity_search_with_score(query, k=top_k) 
            print(f"--- [ì§„ë‹¨ 3/5] ê²€ìƒ‰ ì™„ë£Œ. {len(results)}ê°œì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ---")
            all_results.extend(results)

    unique_results = {}
    for doc, score in all_results:
        if doc.page_content not in unique_results or score < unique_results[doc.page_content][1]:
            unique_results[doc.page_content] = (doc, score)

    sorted_results = sorted(unique_results.values(), key=lambda item: item[1])
    print(f"--- [ì§„ë‹¨ 4/5] ì¤‘ë³µ ì œê±° í›„ {len(sorted_results)}ê°œì˜ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. ---")

    context = ""
    field_names = set()
    for doc, score in sorted_results[:top_k]:
        context += f"- ë‚´ìš©: {doc.page_content}\n"
        field_names.update(doc.metadata.keys())
        # ë””ë²„ê¹…ìš© ë¡œê·¸ ì¶œë ¥ (ì´ì „ ë‹µë³€ì—ì„œ ì¶”ê°€í–ˆë˜ ë‚´ìš©)
        print(f"  [ğŸ” Score] ì ìˆ˜: {score:.4f}, ë‚´ìš© í”„ë¦¬ë·°: {doc.page_content[:50]}...")


    if not context:
        print("!!!!!!!!!!!!!! [ì§„ë‹¨ ê²°ê³¼] ìµœì¢… ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ !!!!!!!!!!!!!!")
    else:
        print(f"--- [ì§„ë‹¨ 5/5] ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ. ---")

    return context, list(field_names)

# --- 4. ì¡¸ì—… ìš”ê±´ ê²€ìƒ‰ í•¨ìˆ˜ (get_graduation_infoëŠ” ë³€ê²½ ì—†ìŒ) ---
# ... (get_graduation_info í•¨ìˆ˜ëŠ” ì´ì „ì— ì£¼ì‹  ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€ë©ë‹ˆë‹¤.)